{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gpytorch\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "from utils.tools import StandardScaler\n",
    "\n",
    "#test Razvan's model\n",
    "from models.ours import PA\n",
    "\n",
    "def setup_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "# set seed\n",
    "setup_seed(42)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "\n",
    "pre_horizon = 12\n",
    "seq_len = pre_horizon\n",
    "pred_len = 1\n",
    "num_nodes = 9\n",
    "\n",
    "data1 = pd.read_csv(\"IMV_LSTM/SML2010/NEW-DATA-1.T15.txt\", sep=' ')\n",
    "data2 = pd.read_csv(\"IMV_LSTM/SML2010/NEW-DATA-2.T15.txt\", sep=' ')\n",
    "target = '3:Temperature_Comedor_Sensor'\n",
    "\n",
    "'''\n",
    "cols = [\n",
    "    '3:Temperature_Comedor_Sensor',\n",
    " '4:Temperature_Habitacion_Sensor',\n",
    " '5:Weather_Temperature',\n",
    " '6:CO2_Comedor_Sensor',\n",
    " '7:CO2_Habitacion_Sensor',\n",
    " '8:Humedad_Comedor_Sensor',\n",
    " '9:Humedad_Habitacion_Sensor',\n",
    " '10:Lighting_Comedor_Sensor',\n",
    " '11:Lighting_Habitacion_Sensor',\n",
    " '12:Precipitacion',\n",
    " '13:Meteo_Exterior_Crepusculo',\n",
    " '14:Meteo_Exterior_Viento',\n",
    " '15:Meteo_Exterior_Sol_Oest',\n",
    " '16:Meteo_Exterior_Sol_Est',\n",
    " '20:Exterior_Entalpic_2',\n",
    " '21:Exterior_Entalpic_turbo',\n",
    " '22:Temperature_Exterior_Sensor']\n",
    "'''\n",
    "\n",
    "\n",
    "cols = [\n",
    "    '3:Temperature_Comedor_Sensor',\n",
    " '4:Temperature_Habitacion_Sensor',\n",
    " '5:Weather_Temperature',\n",
    " '6:CO2_Comedor_Sensor',\n",
    " '7:CO2_Habitacion_Sensor',\n",
    " '8:Humedad_Comedor_Sensor',\n",
    " '9:Humedad_Habitacion_Sensor',\n",
    " '10:Lighting_Comedor_Sensor',\n",
    " '11:Lighting_Habitacion_Sensor']\n",
    "\n",
    "\n",
    "df_data = pd.concat([data1, data2])[cols[:num_nodes]]\n",
    "train_split = 3200\n",
    "test_split = train_split+537\n",
    "train_data = df_data[:train_split]\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "data = (df_data - mean)/std\n",
    "data = torch.Tensor(data.values)\n",
    "print(data.shape)\n",
    "#data = torch.from_numpy(data).unsqueeze(-1) ## unsqueeze(-1)\n",
    "new_data = data.unfold(0, seq_len+pred_len, 1).transpose(1,2)\n",
    "train = new_data[:train_split-pre_horizon][::12]\n",
    "test = new_data[train_split-pre_horizon:test_split-pre_horizon]\n",
    "val = new_data[test_split-pre_horizon:]\n",
    "\n",
    "train_x = train[:,:seq_len,]\n",
    "train_y = train[:,seq_len:,].squeeze(-2)\n",
    "test_x = test[:,:seq_len]\n",
    "test_y = test[:,seq_len:,].squeeze(-2)\n",
    "val_x = val[:,:seq_len]\n",
    "val_y = val[:,seq_len:,].squeeze(-2)\n",
    "\n",
    "full_train_i = torch.full((train_x.shape[0],1), dtype=torch.long, fill_value=0) ##\n",
    "for i in range(1,train_x.shape[-1]):\n",
    "    train_i_task = torch.full((train_x.shape[0],1), dtype=torch.long, fill_value=i)##\n",
    "    full_train_i = torch.cat([full_train_i, train_i_task], 1) ##\n",
    "full_train_i = full_train_i.reshape(-1).cuda()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "    test = test.cuda()\n",
    "    val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "    \n",
    "feature_extractor = PA(horizon=pred_len, lag=seq_len, dynamic=False, supports=None,\n",
    "                       patch_sizes=[1], channels=32, num_nodes=num_nodes, \n",
    "                       input_dim=1, output_dim=1, device='cuda:0')\n",
    "\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape, full_train_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add589f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "        \n",
    "base_kernel_set = [gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()),\\\n",
    "                   gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel()),\\\n",
    "                   gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel()),\\\n",
    "                   gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())]\n",
    "R = 1\n",
    "\n",
    "final_kernel = 0\n",
    "for i in range(R):\n",
    "    #this_iter = base_kernel_set.copy()\n",
    "    add_kernel = copy.deepcopy(base_kernel_set[0])\n",
    "    for j in range(1,len(base_kernel_set)):\n",
    "        add_kernel += copy.deepcopy(base_kernel_set[j])\n",
    "    add_kenrel = gpytorch.kernels.ScaleKernel(add_kernel)\n",
    "\n",
    "    if i == 0:\n",
    "        mul_kernel = copy.deepcopy(add_kernel)\n",
    "        final_kernel = copy.deepcopy(mul_kernel)\n",
    "        continue\n",
    "    else:\n",
    "        mul_kernel *= copy.deepcopy(add_kernel)\n",
    "        final_kernel += copy.deepcopy(mul_kernel)\n",
    "\n",
    "\n",
    "print(final_kernel)\n",
    "\n",
    "'''\n",
    "final_kernel =  gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel()) + \\\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) + \\\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel()*gpytorch.kernels.PeriodicKernel()) + \\\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()*gpytorch.kernels.PeriodicKernel()) + \\\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()*gpytorch.kernels.RBFKernel())\n",
    "'''\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c577606",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(42)\n",
    "\n",
    "idx = []\n",
    "# The forward method\n",
    "from gpytorch.models import ApproximateGP\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            #self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-10., 10.)\n",
    "            self.covar_module = final_kernel\n",
    "            self.task_covar_module = gpytorch.kernels.IndexKernel(num_tasks=num_nodes, rank=1)\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "        def forward(self, x, index): \n",
    "            projected_x, _ = self.feature_extractor(x)\n",
    "            projected_x = projected_x.squeeze()\n",
    "            projected_x = projected_x.reshape(-1)\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            covar_i = self.task_covar_module(index) ##\n",
    "            covar = covar_x.mul(covar_i) ##\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar) ##\n",
    "    \n",
    "#likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_nodes) ##\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = MultitaskGPModel((train_x, full_train_i), train_y.reshape(-1), likelihood) ##\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affce3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "training_iterations = 200\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "min_val_loss = 9999\n",
    "flag = 0\n",
    "retrain = 0\n",
    "torch.save(model.state_dict(), \"./saved/Trigp/ETTh1.pth\")\n",
    "original = model.covar_module\n",
    "\n",
    "full_val_i = torch.full((val_x.shape[0],1), dtype=torch.long, fill_value=0) ##\n",
    "for j in range(1,num_nodes): ##\n",
    "    val_i_task = torch.full((val_x.shape[0],1), dtype=torch.long, fill_value=j)##\n",
    "    full_val_i = torch.cat([full_val_i, val_i_task], 1) ##\n",
    "full_val_i = full_val_i.reshape(-1).cuda()##\n",
    "print(full_val_i.shape, val_x.shape)\n",
    "\n",
    "#full_test_i = torch.full((test_x.shape[0],1), dtype=torch.long, fill_value=0) ##\n",
    "#for j in range(1,num_nodes): ##\n",
    "#    test_i_task = torch.full((test_x.shape[0],1), dtype=torch.long, fill_value=j)##\n",
    "#    full_test_i = torch.cat([full_test_i, test_i_task], 1) ##\n",
    "#full_test_i = full_test_i.reshape(-1).cuda()##\n",
    "#print(full_test_i.shape, test_x.shape)\n",
    "            \n",
    "def train():\n",
    "    patience = 100\n",
    "    counter = 0\n",
    "    val_losses = []\n",
    "    \n",
    "    iterator = tqdm.notebook.tqdm(range(training_iterations))\n",
    "    for i in iterator:\n",
    "        # Find optimal model hyperparameters\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        outputs = model(train_x,full_train_i)\n",
    "        # Calc loss and backprop derivatives\n",
    "        train_loss = -mll(outputs, train_y.reshape(-1))\n",
    "        train_loss.backward()\n",
    "        iterator.set_postfix(train_loss=train_loss.item())\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(val_x, full_val_i)\n",
    "            val_loss = -mll(preds, val_y.reshape(-1))\n",
    "            #preds = model(test_x, full_test_i)\n",
    "            #val_loss = -mll(preds, test_y.reshape(-1))\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "        global min_val_loss\n",
    "        if min_val_loss > val_loss and val_loss > 0:\n",
    "            min_val_loss = val_loss\n",
    "            global flag\n",
    "            global original\n",
    "            original = model.covar_module\n",
    "            flag = 1\n",
    "            print(min_val_loss)\n",
    "            if retrain == 1:\n",
    "                print(\"Saving...\")\n",
    "                torch.save(model.state_dict(), \"./saved/Trigp/ETTh1.pth\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter % 5 == 0:\n",
    "                for params in optimizer.param_groups:\n",
    "                    params['lr'] *= 0.9\n",
    "                    print(params['lr'])\n",
    "                    \n",
    "        if counter == patience:\n",
    "            break\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfccfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "## Retrain:\n",
    "retrain = 1\n",
    "constrains = []\n",
    "for constraint_name, constraint in model.named_constraints():\n",
    "    constrains.append(constraint)\n",
    "\n",
    "\n",
    "index = 0\n",
    "d = len(base_kernel_set)\n",
    "a1 = len(base_kernel_set)\n",
    "an = a1+(R-1)*d\n",
    "length = int((a1+an)*R/2)\n",
    "kernel_weights = torch.zeros(length)\n",
    "for param_name, param in model.named_parameters():\n",
    "    if 'covar_module' and 'raw_outputscale' in param_name:\n",
    "        #print(f'Parameter name: {param_name:42}')\n",
    "        param_name = param_name.split('.')\n",
    "        constrain = constrains[index]\n",
    "        kernel_weights[index] = constraint.transform(param)\n",
    "        index += 1\n",
    "        #print(constraint.transform(param))\n",
    "\n",
    "pre = 0\n",
    "now = a1\n",
    "numbers = 1\n",
    "best_kernel_index = torch.zeros((R,numbers)).int()\n",
    "for r in range(R):\n",
    "    weights = kernel_weights[pre:pre+d]\n",
    "    pre = now\n",
    "    now = a1 + a1+(r+1)*d\n",
    "    weights = nn.functional.softmax(weights, dim=0)\n",
    "    _, kernel_index = torch.sort(weights, descending=True)\n",
    "    best_kernel_index[r] = kernel_index[:numbers]\n",
    "\n",
    "#print(best_kernel_index)\n",
    "####################################################################################\n",
    "\n",
    "final_kernel = 0\n",
    "for i in range(R):\n",
    "    add_kernel = copy.deepcopy(base_kernel_set[best_kernel_index[i][0]])\n",
    "    for j in range(1, numbers):\n",
    "        add_kernel += copy.deepcopy(base_kernel_set[best_kernel_index[i][j]])\n",
    "\n",
    "    if i == 0:\n",
    "        mul_kernel = copy.deepcopy(add_kernel)\n",
    "        final_kernel = copy.deepcopy(mul_kernel)\n",
    "        continue\n",
    "    else:\n",
    "        mul_kernel *= copy.deepcopy(add_kernel)\n",
    "        final_kernel += copy.deepcopy(mul_kernel)\n",
    "\n",
    "\n",
    "print(final_kernel)\n",
    "\n",
    "min_val_loss = 9999\n",
    "flag = 0\n",
    "model.covar_module = final_kernel.cuda()\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "#torch.save(model.state_dict(), \"./saved/Trigp/ETTh1.pth\")\n",
    "train()\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "setup_seed(42)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "# Making Predictions\n",
    "MAE = []\n",
    "RMSE = []\n",
    "MAPE = []\n",
    "model.load_state_dict(torch.load(\"./saved/Trigp/ETTh1.pth\"))\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "predictions = 0\n",
    "        \n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    for i in range(pre_horizon):\n",
    "        if i == 0:\n",
    "            test_x = test[i:-pre_horizon+i,:seq_len].clone()\n",
    "        test_y = test[i:-pre_horizon+i,seq_len].squeeze().clone()\n",
    "        \n",
    "        full_test_i = torch.full((test_x.shape[0],1), dtype=torch.long, fill_value=0) ##\n",
    "        for j in range(1,num_nodes): ##\n",
    "            test_i_task = torch.full((test_x.shape[0],1), dtype=torch.long, fill_value=j)##\n",
    "            full_test_i = torch.cat([full_test_i, test_i_task], 1) ##\n",
    "        full_test_i = full_test_i.reshape(-1).cuda()##\n",
    "        preds = model(test_x,full_test_i)\n",
    "        print(test_x.shape, preds.mean.reshape(-1,num_nodes).unsqueeze(1).shape)\n",
    "        \n",
    "        real_preds = preds.mean[0::num_nodes]\n",
    "        real_test_y = test_y[:,0]\n",
    "        \n",
    "        #print(real_preds.shape)\n",
    "        #print(\"real_test_y\", real_test_y.shape)\n",
    "        real_test_y = real_test_y.cpu()*std[0]+mean[0]\n",
    "        real_preds = real_preds.cpu()*std[0]+mean[0]\n",
    "            \n",
    "        rmse = mean_squared_error(real_test_y.cpu(), real_preds.cpu())\n",
    "        mae = mean_absolute_error(real_test_y.cpu(), real_preds.cpu())\n",
    "        mape = mean_absolute_percentage_error(real_test_y.cpu(), real_preds.cpu())\n",
    "        MAE.append(mae)\n",
    "        RMSE.append(rmse)\n",
    "        MAPE.append(mape)\n",
    "        test_x = torch.cat([test_x.squeeze().clone(), preds.mean.reshape(-1,num_nodes).unsqueeze(1)], axis=1)\n",
    "        test_x = test_x[:,1:,:].clone()\n",
    "        \n",
    "        \n",
    "MAE = np.array(MAE)\n",
    "RMSE = np.array(RMSE)**0.5\n",
    "MAPE = np.array(MAPE)\n",
    "\n",
    "print(MAE.mean(), RMSE.mean(), MAPE.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
